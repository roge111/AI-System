# Немного теории о Системах искусственного интеллекта

Здесь я хочу сделать свою статью про методы машинного обучения. Постараюсь все объяснить максимально просто. Примеры буду приводить на простом языка программирования `Python`.

![](https://media.tenor.com/NeaT_0PBOzQAAAAM/robot-reaction-eww.gif)

# Введение

В наше время всё больше и больше в жизни присутствует искусственный интеллект. От каких-то маленьких игрушин до ИИ-ассистентов. Яндекс Станция, ChatGPT, DeepSeek, YaGPT, GigaChat. Таких помощников становится всё больше.
Но как они работают? Наверняка вы уже слышали, что ИИ работает на основе нейросетей. А нейросети подвергаются обучению. Обучение идет на основе определенного набора данных и результата. Например, есть картинки котов и собак, которые имеют подписи соответствующие.
А потом нейросети подается фото кота. И нейросеть думает: «Значит, это похоже на те фото, которые ранее чаще были подписаны как кот», и выдает результат. Но если нейросеть ошибается, то запоминает результат. Данные, по которым идет обучение, — `обучающая выборка`.
И есть несколько методов обучения. Вот, некоторые:

- линейная регрессия
- k-ближайших соседей
- дерево решений
- логическая регрессия

# Линейная регрессия

Линейная реграссия - один из самых простых и распрастраненных методов обучения. Идея в том, что у нас есть некоторые данные. Метод проводит анализ данных. И на их основе предполагает свое мнение. Например, у нас есть пара X и Y, где X - площадь квратиры, Y - цена.
Таких пар у нас несколько. И на их основе метод пытается построить некую прямую, приблежнную к исходным результатм.  Если говорить просто - найти закономернось в обучающей выборке.

Выглядет это так:

![](https://habrastorage.org/files/619/e50/5fd/619e505fd9a043e3aaba9c91d5e8f0e1.png)

Линия - это наша линейная регрессия, а точки - выборка. А как работает внутри? Давайте разберемся.

Мы хотим, чтобы разница между реальным значением Y и предсказанным была как можно меньше. Такая разница называется `ошибкой`. Для этого используется метода наименьших квадратов с формулой:

$$ SSE = \left( \sum_{i=1}^n (Y_i - (a * X_i + b))^2 \right)$$

Где:
- $Y_i$ - реальное значение
- $a * X_i + b$ - предсказанное значение
-  n - количество точек данных
-  a - наклон
-  b - сдвиг

Как найти a и b ?
--------------------------------------------

a = $( n* ∑(X_i * Y_i) - ∑ X_i * ∑ Y_i)/(n * ∑ (X_i) ^ 2 - ( ∑X_i)^2$

​
b = $(∑Y_i - a * ∑X_i)/n$
​

# Примире на Python

```
import numpy as np

# Данные
X = np.array([50, 60, 70, 80, 90])
Y = np.array([200, 240, 280, 320, 360])

# Вычисляем коэффициенты
n = len(X)
a = (n * np.sum(X * Y) - np.sum(X) * np.sum(Y)) / (n * np.sum(X**2) - np.sum(X)**2)
b = (np.sum(Y) - a * np.sum(X)) / n

print(f"Наклон (a): {a}")
print(f"Сдвиг (b): {b}")

# Предсказание для площади 60
X_new = 60
Y_pred = a * X_new + b
print(f"Предсказанная цена для площади {X_new} м²: {Y_pred}")
```

Дополнительно можно сделать визуализацию

```
import matplotlib.pyplot as plt

# Визуализация
plt.scatter(X, Y, color='blue', label='Данные')  # Точки данных
plt.plot(X, a * X + b, color='red', label='Линия регрессии')  # Линия регрессии
plt.scatter(X_new, Y_pred, color='green', label='Предсказание')  # Предсказание
plt.xlabel('Площадь (м²)')
plt.ylabel('Цена ($)')
plt.legend()
plt.show()
```

Разобрались ?

Пойдем дальше.

![](https://media.tenor.com/kbQv5cIBq8IAAAAM/the-office-steve-carell.gif)

# Метод k-ближайших соседей

Ну не все же так страшно. Или страшно? Если что-то не понятно, то ниже прикреплю материалы. 

Метод k-ближайших соседей (k-NN) - это простейший метод и интуитивно понятный. Тут не так страшно. Логика такого метода очень проста. М смотрим на k ближайших элементов (соседей) и на их основе даем результат. Метод приним как для задач классификации, так и для регрессии.
Если говорить про задачу классификаци - то мы смотрим на k-ближайших соседей и смотрим, что чаще встречается. А если говорить про регерессию, то мы вычисляем медиану.

Вот как это можно реализовать на `Python`


---

### Реализация k-NN с нуля
Мы создадим класс `KNN`, который будет:
1. Принимать данные для обучения.
2. Вычислять расстояния между объектами.
3. Находить `k` ближайших соседей.
4. Делать предсказания (для классификации и регрессии).

---

### Код реализации

```python
import numpy as np
from collections import Counter

class KNN:
    def __init__(self, k=3):
        self.k = k  # Количество соседей

    def fit(self, X_train, y_train):
        """
        Обучение модели (просто запоминаем данные).
        """
        self.X_train = X_train
        self.y_train = y_train

    def predict(self, X_test):
        """
        Предсказание для новых данных.
        """
        predictions = [self._predict(x) for x in X_test]
        return np.array(predictions)

    def _predict(self, x):
        """
        Предсказание для одного объекта.
        """
        # 1. Вычисляем расстояния до всех объектов в обучающей выборке
        distances = [self._euclidean_distance(x, x_train) for x_train in self.X_train]

        # 2. Находим k ближайших соседей
        k_indices = np.argsort(distances)[:self.k]  # Индексы k ближайших соседей
        k_nearest_labels = [self.y_train[i] for i in k_indices]  # Метки k ближайших соседей

        # 3. Для классификации: возвращаем наиболее часто встречающийся класс
        most_common = Counter(k_nearest_labels).most_common(1)
        return most_common[0][0]

    def _euclidean_distance(self, x1, x2):
        """
        Вычисление евклидова расстояния между двумя точками.
        """
        return np.sqrt(np.sum((x1 - x2) ** 2))

# Пример использования
if __name__ == "__main__":
    # Данные для обучения
    X_train = np.array([[1, 2], [2, 3], [3, 3], [6, 7], [7, 8], [8, 8]])
    y_train = np.array([0, 0, 0, 1, 1, 1])

    # Создаём модель
    knn = KNN(k=3)
    knn.fit(X_train, y_train)

    # Новый объект для предсказания
    X_test = np.array([[5, 5]])
    prediction = knn.predict(X_test)

    print(f"Предсказанный класс для {X_test[0]}: {prediction[0]}")
```

---

### Объяснение кода
1. **Класс `KNN`**:
   - `__init__`: Инициализирует количество соседей `k`.
   - `fit`: Запоминает обучающие данные.
   - `predict`: Предсказывает класс для новых данных.
   - `_predict`: Предсказывает класс для одного объекта.
   - `_euclidean_distance`: Вычисляет евклидово расстояние между двумя точками.

2. **Как работает**:
   - Для каждого нового объекта вычисляются расстояния до всех объектов в обучающей выборке.
   - Находятся `k` ближайших соседей.
   - Для классификации выбирается наиболее часто встречающийся класс среди соседей.

3. **Пример**:
   - Обучающие данные: 6 объектов с двумя признаками и метками классов `0` и `1`.
   - Новый объект: `[5, 5]`.
   - Алгоритм находит 3 ближайших соседа и предсказывает класс.

---

### Вывод программы
```
Предсказанный класс для [5 5]: 1
```

---

### Реализация для регрессии
Если нужно реализовать k-NN для регрессии, достаточно изменить метод `_predict`, чтобы он возвращал среднее значение целевой переменной у `k` ближайших соседей:

```python
def _predict(self, x):
    """
    Предсказание для одного объекта (регрессия).
    """
    # 1. Вычисляем расстояния до всех объектов в обучающей выборке
    distances = [self._euclidean_distance(x, x_train) for x_train in self.X_train]

    # 2. Находим k ближайших соседей
    k_indices = np.argsort(distances)[:self.k]  # Индексы k ближайших соседей
    k_nearest_labels = [self.y_train[i] for i in k_indices]  # Значения k ближайших соседей

    # 3. Возвращаем среднее значение
    return np.mean(k_nearest_labels)
```

---


Ну, вроде разобрались. Можно дальше.

# Дерево решений

Тут немного запутанее, но постараюсь объяснить.

У нас есть дерево, где узел - это признак, а ребро - значение. Например

```
    
                [Цвет?]
               /       \
       Красный          Оранжевый
         /                 \
   [Размер?]              Апельсин
   /       \
Большой    Маленький
  |           |
Яблоко      Яблоко

```

И решение считается самый нижний лепесток.

Допустим, у тебя есть новый фрукт:

Цвет: Красный.

Размер: Большой.

Ты спрашиваешь: «Какой цвет?» — «Красный».

Переходишь к следующему вопросу: «Какой размер?» — «Большой».

Лист говорит: «Это яблоко».

Но чтобы так работать, нам надо разбить признаки на группы. Для того чтобы лучше выбрать признак, который лучше всего разделяет на группы, использую метрики:

- Энтропия (метрика неопределенности)
- Индекс Джини (мера неоднородности)
- Прирост информации (насколько уменьшается неопределенность после разделения).

Энтропия — это понятие из теории информации, которое используется в деревьях решений для измерения **неопределенности** или **хаоса** в данных. Чем выше энтропия, тем больше неразбериха в данных, и наоборот — чем она ниже, тем данные более "упорядочены".

---

### Основная идея
Энтропия помогает определить, насколько хорошо признак разделяет данные на группы. Если после разделения энтропия уменьшается, значит, разделение было полезным.

---

### Формула энтропии
Для набора данных с \( C \) классами энтропия вычисляется по формуле:

$\[
H(S) = -\sum_{i=1}^{C} p_i \log_2(p_i)
\]$

Где:
- $\( S \)$ — набор данных,
- $\( p_i \)$ — доля объектов $\( i \)$-го класса в наборе $\( S \)$,
- $\( \log_2 \$) — логарифм по основанию 2.

---

### Как это работает?
1. **Если все объекты принадлежат одному классу**:
   - Энтропия равна 0, потому что нет неопределенности.
   - Пример: $\( p_1 = 1 \)$, $\( p_2 = 0 \)$.
   - $\( H(S) = - (1 \cdot \log_2(1) + 0 \cdot \log_2(0)) = 0 \)$.

2. **Если объекты равномерно распределены между классами**:
   - Энтропия максимальна, потому что неопределенность наибольшая.
   - Пример: $\( p_1 = 0.5 \)$, $\( p_2 = 0.5 \)$.
   - $\( H(S) = - (0.5 \cdot \log_2(0.5) + 0.5 \cdot \log_2(0.5)) = 1 \)$.

3. **В промежуточных случаях**:
   - Энтропия принимает значения между 0 и 1.

---

### Пример вычисления энтропии
Допустим, у нас есть набор данных из 10 объектов:
- 7 объектов принадлежат классу **Яблоко**,
- 3 объекта принадлежат классу **Апельсин**.

1. Вычислим доли:
   - $\( p_{\text{Яблоко}} = \frac{7}{10} = 0.7 \)$,
   - $\( p_{\text{Апельсин}} = \frac{3}{10} = 0.3 \)$.

2. Подставим в формулу:
   $\[
   H(S) = - (0.7 \cdot \log_2(0.7) + 0.3 \cdot \log_2(0.3))
   \]$

3. Вычислим:
   - $\( \log_2(0.7) \approx -0.514 \)$,
   - $\( \log_2(0.3) \approx -1.737 \)$.

4. Итог:
   $\[
   H(S) = - (0.7 \cdot -0.514 + 0.3 \cdot -1.737) \approx 0.881
   \]$

Энтропия этого набора данных равна примерно **0.881**.

---

### Как энтропия используется в деревьях решений?
1. **Вычисление энтропии для исходного набора данных**:
   - Это начальная неопределенность.

2. **Вычисление энтропии после разделения данных**:
   - Для каждого возможного разделения (по каждому признаку) вычисляется энтропия для каждой подгруппы.

3. **Вычисление прироста информации (Information Gain)**:
   - Прирост информации показывает, насколько уменьшилась энтропия после разделения.
   - Формула:
     $\[
     IG(S, A) = H(S) - \sum_{v \in \text{значения признака } A} \frac{|S_v|}{|S|} H(S_v)
     \]$
     Где:
     - $\( S_v \)$ — подгруппа данных для значения $\( v \)$ признака $\( A \)$,
     - $\( |S_v| \)$ — количество объектов в подгруппе $\( S_v \)$,
     - $\( |S| \)$ — общее количество объектов.

4. **Выбор лучшего признака**:
   - Признак с максимальным приростом информации выбирается для разделения.

---

### Пример на Python
Давай вычислим энтропию для нашего примера с фруктами.

```python
import numpy as np

def entropy(y):
    """
    Вычисляет энтропию для набора данных.
    """
    _, counts = np.unique(y, return_counts=True)  # Считаем количество объектов каждого класса
    probabilities = counts / len(y)  # Вычисляем доли
    return -np.sum(probabilities * np.log2(probabilities))  # Формула энтропии

# Данные: 7 яблок, 3 апельсина
y = np.array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])  # 0 — яблоко, 1 — апельсин

# Вычисляем энтропию
print(f"Энтропия: {entropy(y):.3f}")
```

---

### Вывод программы
```
Энтропия: 0.881
```

---

### Полноценный пример реализации дерева решений

```
mport numpy as np

class DecisionTree:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth  # Максимальная глубина дерева
        self.tree = None  # Структура дерева

    def fit(self, X, y, depth=0):
        """
        Обучение дерева решений.
        """
        n_samples, n_features = X.shape
        n_classes = len(np.unique(y))

        # Условия остановки рекурсии
        if (self.max_depth is not None and depth >= self.max_depth) or n_classes == 1:
            return np.argmax(np.bincount(y))  # Возвращаем самый частый класс

        # Ищем лучший признак для разделения
        best_feature, best_threshold = self._best_split(X, y)

        if best_feature is None:
            return np.argmax(np.bincount(y))  # Если разделить нельзя, возвращаем самый частый класс

        # Разделяем данные
        left_indices = X[:, best_feature] <= best_threshold
        right_indices = X[:, best_feature] > best_threshold

        # Рекурсивно строим левое и правое поддеревья
        left_subtree = self.fit(X[left_indices], y[left_indices], depth + 1)
        right_subtree = self.fit(X[right_indices], y[right_indices], depth + 1)

        # Сохраняем структуру дерева
        self.tree = {
            "feature": best_feature,
            "threshold": best_threshold,
            "left": left_subtree,
            "right": right_subtree
        }

        return self.tree

    def _best_split(self, X, y):
        """
        Ищет лучший признак и порог для разделения данных.
        """
        n_samples, n_features = X.shape
        best_feature, best_threshold = None, None
        best_info_gain = -1

        # Перебираем все признаки
        for feature in range(n_features):
            thresholds = np.unique(X[:, feature])  # Все уникальные значения признака

            # Перебираем все пороги
            for threshold in thresholds:
                # Разделяем данные
                left_indices = X[:, feature] <= threshold
                right_indices = X[:, feature] > threshold

                if len(left_indices) == 0 or len(right_indices) == 0:
                    continue  # Пропускаем, если одна из подгрупп пуста

                # Вычисляем прирост информации
                info_gain = self._information_gain(y, y[left_indices], y[right_indices])

                # Обновляем лучший признак и порог
                if info_gain > best_info_gain:
                    best_info_gain = info_gain
                    best_feature = feature
                    best_threshold = threshold

        return best_feature, best_threshold

    def _information_gain(self, y, y_left, y_right):
        """
        Вычисляет прирост информации после разделения.
        """
        # Энтропия до разделения
        entropy_parent = self._entropy(y)

        # Энтропия после разделения
        n_left, n_right = len(y_left), len(y_right)
        n_total = n_left + n_right
        entropy_children = (n_left / n_total) * self._entropy(y_left) + (n_right / n_total) * self._entropy(y_right)

        # Прирост информации
        return entropy_parent - entropy_children

    def _entropy(self, y):
        """
        Вычисляет энтропию для набора данных.
        """
        _, counts = np.unique(y, return_counts=True)
        probabilities = counts / len(y)
        return -np.sum(probabilities * np.log2(probabilities + 1e-10))  # Добавляем маленькое число для стабильности

    def predict(self, X):
        """
        Предсказывает класс для новых данных.
        """
        return np.array([self._predict(x, self.tree) for x in X])

    def _predict(self, x, tree):
        """
        Рекурсивно предсказывает класс для одного объекта.
        """
        if not isinstance(tree, dict):
            return tree  # Если это лист, возвращаем класс

        feature, threshold = tree["feature"], tree["threshold"]
        if x[feature] <= threshold:
            return self._predict(x, tree["left"])  # Идём в левое поддерево
        else:
            return self._predict(x, tree["right"])  # Идём в правое поддерево

# Пример использования
if __name__ == "__main__":
    # Данные: [Цвет, Размер]
    # 0 — красный, 1 — оранжевый
    # 0 — маленький, 1 — большой
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([0, 0, 1, 1])  # 0 — яблоко, 1 — апельсин

    # Создаём и обучаем дерево
    tree = DecisionTree(max_depth=3)
    tree.fit(X, y)

    # Предсказание для новых данных
    new_X = np.array([[0, 1], [1, 0]])  # Новые объекты
    predictions = tree.predict(new_X)
    print(f"Предсказанные классы: {predictions}")
```

Давай разберём код построчно и подробно объясним, как работает каждый метод и блок.

---

### Класс `DecisionTree`

#### 1. Инициализация (`__init__`)
```python
def __init__(self, max_depth=None):
    self.max_depth = max_depth  # Максимальная глубина дерева
    self.tree = None  # Структура дерева
```
- **`max_depth`**: Максимальная глубина дерева. Если `None`, дерево будет расти до тех пор, пока не разделит все данные.
- **`tree`**: Здесь будет храниться структура дерева (словарь с узлами).

---

#### 2. Обучение дерева (`fit`)
```python
def fit(self, X, y, depth=0):
    n_samples, n_features = X.shape
    n_classes = len(np.unique(y))
```
- **`X`**: Массив признаков (например, цвет и размер фруктов).
- **`y`**: Массив целевых значений (классы, например, яблоко или апельсин).
- **`depth`**: Текущая глубина дерева (начинается с 0).

---

#### Условия остановки рекурсии
```python
if (self.max_depth is not None and depth >= self.max_depth) or n_classes == 1:
    return np.argmax(np.bincount(y))  # Возвращаем самый частый класс
```
- Если достигнута максимальная глубина (`depth >= max_depth`) или все объекты в узле принадлежат одному классу (`n_classes == 1`), возвращаем самый частый класс в этом узле.

---

#### Поиск лучшего разделения
```python
best_feature, best_threshold = self._best_split(X, y)
```
- Вызываем метод `_best_split`, чтобы найти лучший признак и порог для разделения данных.

---

#### Проверка на возможность разделения
```python
if best_feature is None:
    return np.argmax(np.bincount(y))  # Если разделить нельзя, возвращаем самый частый класс
```
- Если не удалось найти подходящий признак для разделения (например, все объекты одинаковые), возвращаем самый частый класс.

---

#### Разделение данных
```python
left_indices = X[:, best_feature] <= best_threshold
right_indices = X[:, best_feature] > best_threshold
```
- Разделяем данные на две подгруппы:
  - Левая подгруппа: объекты, у которых значение признака `best_feature` меньше или равно `best_threshold`.
  - Правая подгруппа: объекты, у которых значение признака `best_feature` больше `best_threshold`.

---

#### Рекурсивное построение поддеревьев
```python
left_subtree = self.fit(X[left_indices], y[left_indices], depth + 1)
right_subtree = self.fit(X[right_indices], y[right_indices], depth + 1)
```
- Рекурсивно вызываем `fit` для левой и правой подгрупп, увеличивая глубину на 1.

---

#### Сохранение структуры дерева
```python
self.tree = {
    "feature": best_feature,
    "threshold": best_threshold,
    "left": left_subtree,
    "right": right_subtree
}
```
- Сохраняем структуру дерева в виде словаря:
  - `feature`: Признак, по которому происходит разделение.
  - `threshold`: Порог для разделения.
  - `left`: Левое поддерево.
  - `right`: Правое поддерево.

---

### Метод `_best_split`

#### Инициализация
```python
n_samples, n_features = X.shape
best_feature, best_threshold = None, None
best_info_gain = -1
```
- **`best_feature`**: Лучший признак для разделения.
- **`best_threshold`**: Лучший порог для разделения.
- **`best_info_gain`**: Максимальный прирост информации.

---

#### Перебор всех признаков и порогов
```python
for feature in range(n_features):
    thresholds = np.unique(X[:, feature])  # Все уникальные значения признака
```
- Для каждого признака находим все уникальные значения (пороги).

---

#### Разделение данных
```python
left_indices = X[:, feature] <= threshold
right_indices = X[:, feature] > threshold
```
- Разделяем данные на две подгруппы по текущему порогу.

---

#### Проверка на пустые подгруппы
```python
if len(left_indices) == 0 or len(right_indices) == 0:
    continue  # Пропускаем, если одна из подгрупп пуста
```
- Если одна из подгрупп пуста, пропускаем это разделение.

---

#### Вычисление прироста информации
```python
info_gain = self._information_gain(y, y[left_indices], y[right_indices])
```
- Вычисляем прирост информации для текущего разделения.

---

#### Обновление лучшего разделения
```python
if info_gain > best_info_gain:
    best_info_gain = info_gain
    best_feature = feature
    best_threshold = threshold
```
- Если прирост информации больше текущего лучшего, обновляем лучший признак и порог.

---

### Метод `_information_gain`

#### Формула прироста информации
```python
entropy_parent = self._entropy(y)  # Энтропия до разделения
entropy_children = (n_left / n_total) * self._entropy(y_left) + (n_right / n_total) * self._entropy(y_right)
return entropy_parent - entropy_children  # Прирост информации
```
- Прирост информации = Энтропия до разделения - Взвешенная энтропия после разделения.

---

### Метод `_entropy`

#### Формула энтропии
```python
_, counts = np.unique(y, return_counts=True)
probabilities = counts / len(y)
return -np.sum(probabilities * np.log2(probabilities + 1e-10))
```
- Энтропия вычисляется как:
  \[
  H(S) = -\sum_{i=1}^{C} p_i \log_2(p_i)
  \]
- Добавляем `1e-10` для стабильности (чтобы избежать `log(0)`).

---

### Метод `predict`

#### Предсказание для всех объектов
```python
return np.array([self._predict(x, self.tree) for x in X])
```
- Для каждого объекта вызываем метод `_predict`.

---

### Метод `_predict`

#### Рекурсивное предсказание
```python
if not isinstance(tree, dict):
    return tree  # Если это лист, возвращаем класс

feature, threshold = tree["feature"], tree["threshold"]
if x[feature] <= threshold:
    return self._predict(x, tree["left"])  # Идём в левое поддерево
else:
    return self._predict(x, tree["right"])  # Идём в правое поддерево
```
- Если текущий узел — это лист (не словарь), возвращаем класс.
- Иначе рекурсивно идём в левое или правое поддерево в зависимости от значения признака.

---

### Пример использования

#### Данные
```python
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 0, 1, 1])  # 0 — яблоко, 1 — апельсин
```
- Признаки: цвет (0 — красный, 1 — оранжевый) и размер (0 — маленький, 1 — большой).
- Классы: 0 — яблоко, 1 — апельсин.

---

#### Обучение и предсказание
```python
tree = DecisionTree(max_depth=3)
tree.fit(X, y)

new_X = np.array([[0, 1], [1, 0]])  # Новые объекты
predictions = tree.predict(new_X)
print(f"Предсказанные классы: {predictions}")
```
- Предсказанные классы: `[0, 1]`.

---

### Итог
Этот код реализует дерево решений с нуля, включая:
1. Поиск лучшего разделения данных.
2. Рекурсивное построение дерева.
3. Предсказание классов для новых данных.

Это еще не все.

![](https://media.tenor.com/3xoRK7hFE3gAAAAM/patrick-star-spongebob-squarepants.gif)

# Логическая регрессия

Это как линейная, но только логическая 😂😂😂

Конечно тут не так просто. Давайте рассудим. Логическое значение может иметь два состояния: True или False и прочие его выражения в других формах (например, 1 или 0). Собственно и тут метод у нас работает только с двумя значениями: правда/неправда, да/нет . Такие переменне исследования назывваю `качетсвенными`. 
Давай выясним, как это работает.

![](https://media.tenor.com/kbQv5cIBq8IAAAAM/the-office-steve-carell.gif)

Логистическая регрессия — это один из самых популярных алгоритмов машинного обучения, который используется для задач **классификации**. Несмотря на название, это не регрессия, а метод классификации. Он предсказывает вероятность принадлежности объекта к определённому классу.

---

### Основная идея
Логистическая регрессия предсказывает вероятность того, что объект принадлежит к одному из двух классов (бинарная классификация). Например:
- Болеет ли человек определённой болезнью (да/нет).
- Является ли email спамом (да/нет).

---

### Как это работает?
1. **Линейная комбинация признаков**:
   - Логистическая регрессия сначала вычисляет линейную комбинацию признаков:
     $\[
     z = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n
     \]$
     Где:
     - $\( w_0, w_1, \dots, w_n \)$ — веса модели,
     - $\( x_1, x_2, \dots, x_n \)$ — признаки объекта.

2. **Сигмоидная функция**:
   - Чтобы преобразовать линейную комбинацию в вероятность, используется **сигмоидная функция**:
     $\[
     \sigma(z) = \frac{1}{1 + e^{-z}}
     \]$
     Сигмоидная функция "сжимает" значение $\( z \)$ в интервал $\([0, 1]\)$, что можно интерпретировать как вероятность.

3. **Предсказание класса**:
   - Если вероятность $\( \sigma(z) \geq 0.5 \)$, объект относится к классу 1.
   - Если вероятность $\( \sigma(z) < 0.5 \)$, объект относится к классу 0.

---

### Обучение модели
Логистическая регрессия обучается путём минимизации **функции потерь** (Log Loss). Функция потерь измеряет, насколько предсказанные вероятности отличаются от реальных меток.

#### Функция потерь (Log Loss)
$\[
\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\sigma(z_i)) + (1 - y_i) \log(1 - \sigma(z_i)) \right]
\]$
Где:
- $\( y_i \)$ — реальная метка (0 или 1),
- $\( \sigma(z_i) \)$ — предсказанная вероятность,
- $\( N \)$ — количество объектов.

---

### Код на Python

```

class LogisticRegression:
    def __init__(self, learning_rate=0.01, num_iterations=1000):
        self.learning_rate = learning_rate
        self.num_iterations = num_iterations
        self.weights = None
        self.bias = None
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def log_loss(self, y, y_pred):
        return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))
    
    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        for _ in range(self.num_iterations):
            linear_model = np.dot(X, self.weights) + self.bias
            y_pred = self.sigmoid(linear_model)
            
            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))
            db = (1 / n_samples) * np.sum(y_pred - y)
            
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
    
    def predict(self, X):
        linear_model = np.dot(X, self.weights) + self.bias
        y_pred = self.sigmoid(linear_model)
        return [1 if i > 0.5 else 0 for i in y_pred]

```

Конечно! Давай разберём этот код, который реализует **логистическую регрессию с нуля**. Мы поэтапно объясним, что делает каждая часть кода.

---

### Класс `LogisticRegression`

#### 1. Инициализация (`__init__`)
```python
def __init__(self, learning_rate=0.01, num_iterations=1000):
    self.learning_rate = learning_rate
    self.num_iterations = num_iterations
    self.weights = None
    self.bias = None
```
- **`learning_rate`**: Скорость обучения. Определяет, насколько сильно обновляются веса на каждом шаге.
- **`num_iterations`**: Количество итераций обучения.
- **`weights`**: Веса модели (коэффициенты для каждого признака). Инициализируются как `None`.
- **`bias`**: Свободный член (смещение). Инициализируется как `None`.

---

#### 2. Сигмоидная функция (`sigmoid`)
```python
def sigmoid(self, z):
    return 1 / (1 + np.exp(-z))
```
- **Сигмоидная функция** преобразует любое число в интервал $\([0, 1]\)$. Это позволяет интерпретировать результат как вероятность.
- Формула:
  $\[
  \sigma(z) = \frac{1}{1 + e^{-z}}
  \]$

---

#### 3. Функция потерь (`log_loss`)
```python
def log_loss(self, y, y_pred):
    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))
```
- **Функция потерь (Log Loss)** измеряет, насколько предсказанные вероятности отличаются от реальных меток.
- Формула:
  $\[
  \text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
  \]$
  Где:
  - $\( y_i \)$ — реальная метка (0 или 1),
  - $\( \hat{y}_i \)$ — предсказанная вероятность.

---

#### 4. Обучение модели (`fit`)
```python
def fit(self, X, y):
    n_samples, n_features = X.shape
    self.weights = np.zeros(n_features)
    self.bias = 0
```
- **Инициализация весов и смещения**:
  - Веса (`weights`) инициализируются нулями.
  - Смещение (`bias`) инициализируется нулём.

---

#### Градиентный спуск
```python
for _ in range(self.num_iterations):
    linear_model = np.dot(X, self.weights) + self.bias
    y_pred = self.sigmoid(linear_model)
```
- **Линейная комбинация**:
  - Вычисляется линейная комбинация признаков и весов:
    $\[
    z = X \cdot \text{weights} + \text{bias}
    \]$
- **Предсказание**:
  - Линейная комбинация преобразуется в вероятность с помощью сигмоидной функции:
    $\[
    \hat{y} = \sigma(z)
    \]$

---

#### Обновление весов и смещения
```python
dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))
db = (1 / n_samples) * np.sum(y_pred - y)

self.weights -= self.learning_rate * dw
self.bias -= self.learning_rate * db
```
- **Градиенты**:
  - `dw`: Градиент по весам. Показывает, как нужно изменить веса, чтобы уменьшить ошибку.
  - `db`: Градиент по смещению. Показывает, как нужно изменить смещение, чтобы уменьшить ошибку.
- **Обновление весов и смещения**:
  - Веса и смещение обновляются с учётом градиентов и скорости обучения:
    $weights = weights - learning_rate \cdot dw$
    $bias = bias - learning_rate \cdot db$

---

#### 5. Предсказание (`predict`)
```python
def predict(self, X):
    linear_model = np.dot(X, self.weights) + self.bias
    y_pred = self.sigmoid(linear_model)
    return [1 if i > 0.5 else 0 for i in y_pred]
```
- **Линейная комбинация**:
  - Вычисляется линейная комбинация для новых данных:
    $\[
    z = X \cdot \text{weights} + \text{bias}
    \]$
- **Предсказание**:
  - Линейная комбинация преобразуется в вероятность с помощью сигмоидной функции.
  - Если вероятность $\( \hat{y} > 0.5 \)$, предсказывается класс 1, иначе — класс 0.

---

### Пример использования

#### Данные
```python
X = np.array([[25, 40000], [30, 60000], [35, 80000], [40, 100000], [45, 120000]])
y = np.array([0, 0, 1, 1, 1])
```
- Признаки: возраст и зарплата.
- Классы: 0 — не купил, 1 — купил.

---

#### Обучение и предсказание
```python
model = LogisticRegression(learning_rate=0.01, num_iterations=1000)
model.fit(X, y)

new_X = np.array([[50, 140000], [28, 50000]])
predictions = model.predict(new_X)
print(f"Предсказанные классы: {predictions}")
```

---

### Итог
Этот код реализует логистическую регрессию с нуля, включая:
1. Сигмоидную функцию.
2. Функцию потерь (Log Loss).
3. Градиентный спуск для обучения модели.
4. Предсказание классов.

#### Материалы

В первую очередь руководствовался материалами из ![Университета ИТМО](https://books.ifmo.ru/file/pdf/3308.pdf)

