# Немного теории о Системах искусственного интеллекта

Здесь я хочу сделать свою статью про методы машинного обучения. Постараюсь все объяснить максимально просто. Примеры буду приводить на простом языка программирования `Python`.

![](https://media.tenor.com/NeaT_0PBOzQAAAAM/robot-reaction-eww.gif)

# Введение

В наше время всё больше и больше в жизни присутствует искусственный интеллект. От каких-то маленьких игрушин до ИИ-ассистентов. Яндекс Станция, ChatGPT, DeepSeek, YaGPT, GigaChat. Таких помощников становится всё больше.
Но как они работают? Наверняка вы уже слышали, что ИИ работает на основе нейросетей. А нейросети подвергаются обучению. Обучение идет на основе определенного набора данных и результата. Например, есть картинки котов и собак, которые имеют подписи соответствующие.
А потом нейросети подается фото кота. И нейросеть думает: «Значит, это похоже на те фото, которые ранее чаще были подписаны как кот», и выдает результат. Но если нейросеть ошибается, то запоминает результат. Данные, по которым идет обучение, — `обучающая выборка`.
И есть несколько методов обучения. Вот, некоторые:

- линейная регрессия
- k-ближайших соседей
- дерево решений
- логическая регрессия

# Линейная регрессия

Линейная реграссия - один из самых простых и распрастраненных методов обучения. Идея в том, что у нас есть некоторые данные. Метод проводит анализ данных. И на их основе предполагает свое мнение. Например, у нас есть пара X и Y, где X - площадь квратиры, Y - цена.
Таких пар у нас несколько. И на их основе метод пытается построить некую прямую, приблежнную к исходным результатм.  Если говорить просто - найти закономернось в обучающей выборке.

Выглядет это так:

![](https://habrastorage.org/files/619/e50/5fd/619e505fd9a043e3aaba9c91d5e8f0e1.png)

Линия - это наша линейная регрессия, а точки - выборка. А как работает внутри? Давайте разберемся.

Мы хотим, чтобы разница между реальным значением Y и предсказанным была как можно меньше. Такая разница называется `ошибкой`. Для этого используется метода наименьших квадратов с формулой:

$$ SSE = \left( \sum_{i=1}^n (Y_i - (a * X_i + b))^2 \right)$$

Где:
- $Y_i$ - реальное значение
- $a * X_i + b$ - предсказанное значение
-  n - количество точек данных
-  a - наклон
-  b - сдвиг

Как найти a и b ?
--------------------------------------------

a = $( n* ∑(X_i * Y_i) - ∑ X_i * ∑ Y_i)/(n * ∑ (X_i) ^ 2 - ( ∑X_i)^2$

​
b = $(∑Y_i - a * ∑X_i)/n$
​

# Примире на Python

```
import numpy as np

# Данные
X = np.array([50, 60, 70, 80, 90])
Y = np.array([200, 240, 280, 320, 360])

# Вычисляем коэффициенты
n = len(X)
a = (n * np.sum(X * Y) - np.sum(X) * np.sum(Y)) / (n * np.sum(X**2) - np.sum(X)**2)
b = (np.sum(Y) - a * np.sum(X)) / n

print(f"Наклон (a): {a}")
print(f"Сдвиг (b): {b}")

# Предсказание для площади 60
X_new = 60
Y_pred = a * X_new + b
print(f"Предсказанная цена для площади {X_new} м²: {Y_pred}")
```

Дополнительно можно сделать визуализацию

```
import matplotlib.pyplot as plt

# Визуализация
plt.scatter(X, Y, color='blue', label='Данные')  # Точки данных
plt.plot(X, a * X + b, color='red', label='Линия регрессии')  # Линия регрессии
plt.scatter(X_new, Y_pred, color='green', label='Предсказание')  # Предсказание
plt.xlabel('Площадь (м²)')
plt.ylabel('Цена ($)')
plt.legend()
plt.show()
```

Разобрались ?

Пойдем дальше.

![](https://media.tenor.com/kbQv5cIBq8IAAAAM/the-office-steve-carell.gif)

# Метод k-ближайших соседей

Ну не все же так страшно. Или страшно? Если что-то не понятно, то ниже прикреплю материалы. 

Метод k-ближайших соседей (k-NN) - это простейший метод и интуитивно понятный. Тут не так страшно. Логика такого метода очень проста. М смотрим на k ближайших элементов (соседей) и на их основе даем результат. Метод приним как для задач классификации, так и для регрессии.
Если говорить про задачу классификаци - то мы смотрим на k-ближайших соседей и смотрим, что чаще встречается. А если говорить про регерессию, то мы вычисляем медиану.

Вот как это можно реализовать на `Python`


---

### Реализация k-NN с нуля
Мы создадим класс `KNN`, который будет:
1. Принимать данные для обучения.
2. Вычислять расстояния между объектами.
3. Находить `k` ближайших соседей.
4. Делать предсказания (для классификации и регрессии).

---

### Код реализации

```python
import numpy as np
from collections import Counter

class KNN:
    def __init__(self, k=3):
        self.k = k  # Количество соседей

    def fit(self, X_train, y_train):
        """
        Обучение модели (просто запоминаем данные).
        """
        self.X_train = X_train
        self.y_train = y_train

    def predict(self, X_test):
        """
        Предсказание для новых данных.
        """
        predictions = [self._predict(x) for x in X_test]
        return np.array(predictions)

    def _predict(self, x):
        """
        Предсказание для одного объекта.
        """
        # 1. Вычисляем расстояния до всех объектов в обучающей выборке
        distances = [self._euclidean_distance(x, x_train) for x_train in self.X_train]

        # 2. Находим k ближайших соседей
        k_indices = np.argsort(distances)[:self.k]  # Индексы k ближайших соседей
        k_nearest_labels = [self.y_train[i] for i in k_indices]  # Метки k ближайших соседей

        # 3. Для классификации: возвращаем наиболее часто встречающийся класс
        most_common = Counter(k_nearest_labels).most_common(1)
        return most_common[0][0]

    def _euclidean_distance(self, x1, x2):
        """
        Вычисление евклидова расстояния между двумя точками.
        """
        return np.sqrt(np.sum((x1 - x2) ** 2))

# Пример использования
if __name__ == "__main__":
    # Данные для обучения
    X_train = np.array([[1, 2], [2, 3], [3, 3], [6, 7], [7, 8], [8, 8]])
    y_train = np.array([0, 0, 0, 1, 1, 1])

    # Создаём модель
    knn = KNN(k=3)
    knn.fit(X_train, y_train)

    # Новый объект для предсказания
    X_test = np.array([[5, 5]])
    prediction = knn.predict(X_test)

    print(f"Предсказанный класс для {X_test[0]}: {prediction[0]}")
```

---

### Объяснение кода
1. **Класс `KNN`**:
   - `__init__`: Инициализирует количество соседей `k`.
   - `fit`: Запоминает обучающие данные.
   - `predict`: Предсказывает класс для новых данных.
   - `_predict`: Предсказывает класс для одного объекта.
   - `_euclidean_distance`: Вычисляет евклидово расстояние между двумя точками.

2. **Как работает**:
   - Для каждого нового объекта вычисляются расстояния до всех объектов в обучающей выборке.
   - Находятся `k` ближайших соседей.
   - Для классификации выбирается наиболее часто встречающийся класс среди соседей.

3. **Пример**:
   - Обучающие данные: 6 объектов с двумя признаками и метками классов `0` и `1`.
   - Новый объект: `[5, 5]`.
   - Алгоритм находит 3 ближайших соседа и предсказывает класс.

---

### Вывод программы
```
Предсказанный класс для [5 5]: 1
```

---

### Реализация для регрессии
Если нужно реализовать k-NN для регрессии, достаточно изменить метод `_predict`, чтобы он возвращал среднее значение целевой переменной у `k` ближайших соседей:

```python
def _predict(self, x):
    """
    Предсказание для одного объекта (регрессия).
    """
    # 1. Вычисляем расстояния до всех объектов в обучающей выборке
    distances = [self._euclidean_distance(x, x_train) for x_train in self.X_train]

    # 2. Находим k ближайших соседей
    k_indices = np.argsort(distances)[:self.k]  # Индексы k ближайших соседей
    k_nearest_labels = [self.y_train[i] for i in k_indices]  # Значения k ближайших соседей

    # 3. Возвращаем среднее значение
    return np.mean(k_nearest_labels)
```

---


Ну, вроде разобрались. Можно дальше.

# Дерево решений

Тут немного запутанее, но постараюсь объяснить.

У нас есть дерево, где узел - это признак, а ребро - значение. Например

```
    
                [Цвет?]
               /       \
       Красный          Оранжевый
         /                 \
   [Размер?]              Апельсин
   /       \
Большой    Маленький
  |           |
Яблоко      Яблоко

```

И решение считается самый нижний лепесток.

Допустим, у тебя есть новый фрукт:

Цвет: Красный.

Размер: Большой.

Ты спрашиваешь: «Какой цвет?» — «Красный».

Переходишь к следующему вопросу: «Какой размер?» — «Большой».

Лист говорит: «Это яблоко».

Но чтобы так работать, нам надо разбить признаки на группы. Для того чтобы лучше выбрать признак, который лучше всего разделяет на группы, использую метрики:

- Энтропия (метрика неопределенности)
- Индекс Джини (мера неоднородности)
- Прирост информации (насколько уменьшается неопределенность после разделения).

Энтропия — это понятие из теории информации, которое используется в деревьях решений для измерения **неопределенности** или **хаоса** в данных. Чем выше энтропия, тем больше неразбериха в данных, и наоборот — чем она ниже, тем данные более "упорядочены".

---

### Основная идея
Энтропия помогает определить, насколько хорошо признак разделяет данные на группы. Если после разделения энтропия уменьшается, значит, разделение было полезным.

---

### Формула энтропии
Для набора данных с \( C \) классами энтропия вычисляется по формуле:

$\[
H(S) = -\sum_{i=1}^{C} p_i \log_2(p_i)
\]$

Где:
- $\( S \)$ — набор данных,
- $\( p_i \)$ — доля объектов $\( i \)$-го класса в наборе $\( S \)$,
- $\( \log_2 \$) — логарифм по основанию 2.

---

### Как это работает?
1. **Если все объекты принадлежат одному классу**:
   - Энтропия равна 0, потому что нет неопределенности.
   - Пример: $\( p_1 = 1 \)$, $\( p_2 = 0 \)$.
   - $\( H(S) = - (1 \cdot \log_2(1) + 0 \cdot \log_2(0)) = 0 \)$.

2. **Если объекты равномерно распределены между классами**:
   - Энтропия максимальна, потому что неопределенность наибольшая.
   - Пример: $\( p_1 = 0.5 \)$, $\( p_2 = 0.5 \)$.
   - $\( H(S) = - (0.5 \cdot \log_2(0.5) + 0.5 \cdot \log_2(0.5)) = 1 \)$.

3. **В промежуточных случаях**:
   - Энтропия принимает значения между 0 и 1.

---

### Пример вычисления энтропии
Допустим, у нас есть набор данных из 10 объектов:
- 7 объектов принадлежат классу **Яблоко**,
- 3 объекта принадлежат классу **Апельсин**.

1. Вычислим доли:
   - $\( p_{\text{Яблоко}} = \frac{7}{10} = 0.7 \)$,
   - $\( p_{\text{Апельсин}} = \frac{3}{10} = 0.3 \)$.

2. Подставим в формулу:
   $\[
   H(S) = - (0.7 \cdot \log_2(0.7) + 0.3 \cdot \log_2(0.3))
   \]$

3. Вычислим:
   - $\( \log_2(0.7) \approx -0.514 \)$,
   - $\( \log_2(0.3) \approx -1.737 \)$.

4. Итог:
   $\[
   H(S) = - (0.7 \cdot -0.514 + 0.3 \cdot -1.737) \approx 0.881
   \]$

Энтропия этого набора данных равна примерно **0.881**.

---

### Как энтропия используется в деревьях решений?
1. **Вычисление энтропии для исходного набора данных**:
   - Это начальная неопределенность.

2. **Вычисление энтропии после разделения данных**:
   - Для каждого возможного разделения (по каждому признаку) вычисляется энтропия для каждой подгруппы.

3. **Вычисление прироста информации (Information Gain)**:
   - Прирост информации показывает, насколько уменьшилась энтропия после разделения.
   - Формула:
     $\[
     IG(S, A) = H(S) - \sum_{v \in \text{значения признака } A} \frac{|S_v|}{|S|} H(S_v)
     \]$
     Где:
     - $\( S_v \)$ — подгруппа данных для значения $\( v \)$ признака $\( A \)$,
     - $\( |S_v| \)$ — количество объектов в подгруппе $\( S_v \)$,
     - $\( |S| \)$ — общее количество объектов.

4. **Выбор лучшего признака**:
   - Признак с максимальным приростом информации выбирается для разделения.

---

### Пример на Python
Давай вычислим энтропию для нашего примера с фруктами.

```python
import numpy as np

def entropy(y):
    """
    Вычисляет энтропию для набора данных.
    """
    _, counts = np.unique(y, return_counts=True)  # Считаем количество объектов каждого класса
    probabilities = counts / len(y)  # Вычисляем доли
    return -np.sum(probabilities * np.log2(probabilities))  # Формула энтропии

# Данные: 7 яблок, 3 апельсина
y = np.array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])  # 0 — яблоко, 1 — апельсин

# Вычисляем энтропию
print(f"Энтропия: {entropy(y):.3f}")
```

---

### Вывод программы
```
Энтропия: 0.881
```

---

### Итог
Энтропия — это мера неопределенности в данных. В деревьях решений она используется для выбора лучшего признака для разделения данных. Чем больше уменьшается энтропия после разделения, тем лучше выбранный признак. 😊

